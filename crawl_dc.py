import requests
from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import time
import json
import os
from pathlib import Path

# Configuration
BASE_URL = "https://gall.dcinside.com/mgallery/board/lists/"
GALL_ID = "thesingularity"
GALL_NAME = "íŠ¹ì´ì ì´ ì˜¨ë‹¤"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
DAYS_LIMIT = 28  # 4ì£¼ê°„ ë°ì´í„° ìˆ˜ì§‘

# Data directory
DATA_DIR = Path(__file__).parent / "data"
DATA_DIR.mkdir(exist_ok=True)


def get_week_info(date):
    """ISO 8601 ì£¼ì°¨ ì •ë³´ ë°˜í™˜"""
    iso_calendar = date.isocalendar()
    year = iso_calendar[0]
    week = iso_calendar[1]
    
    # ì£¼ì˜ ì‹œì‘ì¼ê³¼ ì¢…ë£Œì¼ ê³„ì‚°
    week_start = date - timedelta(days=date.weekday())
    week_end = week_start + timedelta(days=6)
    
    return {
        'week_id': f"{year}_W{week:02d}",
        'year': year,
        'week': week,
        'week_start': week_start.strftime('%Y-%m-%d'),
        'week_end': week_end.strftime('%Y-%m-%d')
    }


def load_week_data(week_id):
    """ì£¼ì°¨ë³„ JSON íŒŒì¼ ë¡œë“œ"""
    file_path = DATA_DIR / f"{week_id}.json"
    
    if file_path.exists():
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    return None


def save_week_data(week_id, data):
    """ì£¼ì°¨ë³„ JSON íŒŒì¼ ì €ì¥"""
    file_path = DATA_DIR / f"{week_id}.json"
    
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {file_path.name}")


def create_week_structure(week_info):
    """ë¹ˆ ì£¼ì°¨ ë°ì´í„° êµ¬ì¡° ìƒì„±"""
    return {
        "gallery_id": GALL_ID,
        "gallery_name": GALL_NAME,
        "week": week_info['week_id'],
        "week_start": week_info['week_start'],
        "week_end": week_info['week_end'],
        "last_updated": datetime.now().strftime('%Y-%m-%dT%H:%M:%S+09:00'),
        "posts": [],
        "total_posts": 0
    }


def parse_date(date_tag, today):
    """ë‚ ì§œ íƒœê·¸ì—ì„œ datetime ê°ì²´ ì¶”ì¶œ"""
    if not date_tag:
        return None
    
    # title ì†ì„±ì— ì „ì²´ ë‚ ì§œ+ì‹œê°„ ì •ë³´ê°€ ìˆìŒ
    date_str = date_tag.get('title')
    if date_str:
        try:
            return datetime.strptime(date_str, '%Y-%m-%d %H:%M:%S')
        except:
            pass
    
    # Fallback: í™”ë©´ í‘œì‹œ í…ìŠ¤íŠ¸ íŒŒì‹±
    date_text = date_tag.get_text(strip=True)
    
    if ':' in date_text:  # HH:MM (ì˜¤ëŠ˜)
        try:
            hour, minute = map(int, date_text.split(':'))
            return datetime(today.year, today.month, today.day, hour, minute)
        except:
            pass
    elif '.' in date_text:
        parts = date_text.split('.')
        try:
            if len(parts) == 2:  # MM.DD (ì˜¬í•´)
                return datetime(today.year, int(parts[0]), int(parts[1]))
            elif len(parts) == 3:  # YY.MM.DD
                year = 2000 + int(parts[0])
                return datetime(year, int(parts[1]), int(parts[2]))
        except:
            pass
    
    return None


def extract_post_data(row, today):
    """ê²Œì‹œê¸€ í–‰ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    try:
        # ê³µì§€, ë‰´ìŠ¤, ì„¤ë¬¸ ë“± íŠ¹ìˆ˜ ê²Œì‹œê¸€ ì œì™¸
        data_type = row.get('data-type', '')
        if 'icon_notice' in data_type:
            # ê³µì§€, ë‰´ìŠ¤, ì„¤ë¬¸ ë“±ì€ í¬ë¡¤ë§í•˜ì§€ ì•ŠìŒ
            return None
        
        # ì œëª© ë° URL
        title_tag = row.select_one('td.gall_tit a')
        if not title_tag:
            return None
        
        # ëŒ“ê¸€ìˆ˜ ì œê±°
        comment_tag = title_tag.select_one('.reply_numbox')
        comments = 0
        if comment_tag:
            try:
                comments = int(comment_tag.get_text(strip=True).strip('[]'))
            except:
                pass
            comment_tag.decompose()
        
        title = title_tag.get_text(strip=True)
        url = "https://gall.dcinside.com" + title_tag['href']
        
        # ê²Œì‹œê¸€ ë²ˆí˜¸ ì¶”ì¶œ (data-no ì†ì„± ìš°ì„  ì‚¬ìš©)
        post_id = row.get('data-no', '')
        
        if not post_id:
            # fallback: gall_numì—ì„œ ì¶”ì¶œ
            num_td = row.select_one('td.gall_num')
            num_text = num_td.get_text(strip=True) if num_td else ""
            
            if not num_text.isdigit():
                # ìˆ«ìê°€ ì•„ë‹ˆë©´ ê±´ë„ˆë›°ê¸°
                return None
            
            post_id = num_text
        
        # ë‚ ì§œ ë° ì‹œê°„
        date_tag = row.select_one('td.gall_date')
        post_datetime = parse_date(date_tag, today)
        
        if not post_datetime:
            return None
        
        # ì‘ì„±ì ì •ë³´
        writer_tag = row.select_one('td.gall_writer')
        author = ""
        author_type = "unknown"
        author_ip = ""
        
        if writer_tag:
            # data-nick ë˜ëŠ” data-ip ì†ì„±
            author = writer_tag.get('data-nick', '')
            author_ip = writer_tag.get('data-ip', '')
            
            if not author:
                author = writer_tag.get_text(strip=True)
            
            # ì‘ì„±ì ìœ í˜• íŒë³„
            if writer_tag.get('data-uid'):
                author_type = "member"
            elif author_ip:
                if author.startswith('ã…‡ã…‡'):
                    author_type = "semi_anonymous"
                else:
                    author_type = "ip"
            else:
                author_type = "ip"
        
        # ì¡°íšŒìˆ˜
        views = 0
        count_tag = row.select_one('td.gall_count')
        if count_tag:
            try:
                views = int(count_tag.get_text(strip=True))
            except:
                pass
        
        # ì¶”ì²œìˆ˜
        likes = 0
        recommend_tag = row.select_one('td.gall_recommend')
        if recommend_tag:
            try:
                likes = int(recommend_tag.get_text(strip=True))
            except:
                pass
        
        return {
            'post_id': post_id,
            'title': title,
            'author': author,
            'author_ip': author_ip,
            'author_type': author_type,
            'date': post_datetime.strftime('%Y-%m-%d'),
            'time': post_datetime.strftime('%H:%M:%S'),
            'datetime': post_datetime.strftime('%Y-%m-%dT%H:%M:%S'),
            'views': views,
            'likes': likes,
            'comments': comments,
            'url': url,
            '_datetime_obj': post_datetime  # ì •ë ¬ìš© ì„ì‹œ í•„ë“œ
        }
    
    except Exception as e:
        return None


def load_all_existing_ids():
    """ëª¨ë“  ì£¼ì°¨ì˜ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ID ì§‘í•© ë°˜í™˜"""
    all_ids = set()
    for file_path in DATA_DIR.glob("*.json"):
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            for post in data.get('posts', []):
                all_ids.add(post['post_id'])
    return all_ids

def crawl_posts():
    """ê²Œì‹œê¸€ í¬ë¡¤ë§"""
    print("=" * 60)
    print(f"ğŸ” DC í¬ë¡¤ëŸ¬ ì‹œì‘")
    print(f"ê°¤ëŸ¬ë¦¬: {GALL_NAME} ({GALL_ID})")
    print(f"ìˆ˜ì§‘ ê¸°ê°„: ìµœê·¼ {DAYS_LIMIT}ì¼")
    print("=" * 60)
    
    today = datetime.now()
    cutoff_date = today - timedelta(days=DAYS_LIMIT)
    
    # ê¸°ì¡´ì— ìˆ˜ì§‘ëœ ëª¨ë“  ID ë¡œë“œ
    existing_ids = load_all_existing_ids()
    print(f"ğŸ“‚ ê¸°ì¡´ ìˆ˜ì§‘ëœ ê²Œì‹œê¸€: {len(existing_ids)}ê°œ")
    
    all_posts = []
    page = 1
    stop_crawling = False
    new_posts_count = 0
    consecutive_dup_count = 0  # ì—°ì† ì¤‘ë³µ ì¹´ìš´íŠ¸
    
    while not stop_crawling:
        url = f"{BASE_URL}?id={GALL_ID}&page={page}"
        print(f"\rğŸ“„ í˜ì´ì§€ {page} ì²˜ë¦¬ ì¤‘... (ì‹ ê·œ ìˆ˜ì§‘: {new_posts_count}ê°œ)", end='', flush=True)
        
        try:
            response = requests.get(url, headers=HEADERS)
            response.raise_for_status()
        except requests.RequestException as e:
            print(f"\nâŒ í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨: {e}")
            break
        
        soup = BeautifulSoup(response.text, 'html.parser')
        rows = soup.select('tr.ub-content')
        
        if not rows:
            print(f"\nâš ï¸  ê²Œì‹œê¸€ ì—†ìŒ (í˜ì´ì§€ {page})")
            break
        
        for row in rows:
            post_data = extract_post_data(row, today)
            
            if not post_data:
                continue
            
            post_id = post_data['post_id']
            post_datetime = post_data['_datetime_obj']
            
            # 1. ì¤‘ë³µ ì²´í¬ (ì—°ì† 5íšŒ ì´ìƒì´ë©´ ì¤‘ë‹¨)
            if post_id in existing_ids:
                consecutive_dup_count += 1
                if consecutive_dup_count >= 5:
                    stop_crawling = True
                    print(f"\nâœ‹ ì—°ì†ìœ¼ë¡œ 5ê°œì˜ ì¤‘ë³µ ê²Œì‹œê¸€ ë°œê²¬ (ë§ˆì§€ë§‰ ID: {post_id}) â†’ í¬ë¡¤ë§ ì¤‘ë‹¨")
                    break
                continue  # ì´ ê¸€ì€ ê±´ë„ˆë›°ê³  ë‹¤ìŒ ê¸€ í™•ì¸
            else:
                consecutive_dup_count = 0  # ì‹ ê·œ ê¸€ì´ ë‚˜ì˜¤ë©´ ì¹´ìš´íŠ¸ ë¦¬ì…‹
            
            # 2. ë‚ ì§œ ì œí•œ ì²´í¬
            if post_datetime >= cutoff_date:
                all_posts.append(post_data)
                new_posts_count += 1
            else:
                # ì˜¤ë˜ëœ ê²Œì‹œê¸€ ë°œê²¬ â†’ í¬ë¡¤ë§ ì¤‘ë‹¨
                stop_crawling = True
                print(f"\nğŸ“… ìˆ˜ì§‘ ê¸°ê°„ ì´ˆê³¼ ({post_datetime}) â†’ í¬ë¡¤ë§ ì¤‘ë‹¨")
                break
        
        if stop_crawling:
            break
        
        page += 1
        time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
    
    print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì‹ ê·œ {len(all_posts)}ê°œ ìˆ˜ì§‘")
    
    # _datetime_obj í•„ë“œ ì œê±°
    for post in all_posts:
        del post['_datetime_obj']
    
    return all_posts


def organize_by_week(posts):
    """ê²Œì‹œê¸€ì„ ì£¼ì°¨ë³„ë¡œ ë¶„ë¥˜"""
    weeks = {}
    
    for post in posts:
        post_date = datetime.strptime(post['datetime'], '%Y-%m-%dT%H:%M:%S')
        week_info = get_week_info(post_date)
        week_id = week_info['week_id']
        
        if week_id not in weeks:
            weeks[week_id] = {
                'info': week_info,
                'posts': []
            }
        
        weeks[week_id]['posts'].append(post)
    
    return weeks


def merge_and_save(weeks_data):
    """ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•© í›„ ì €ì¥"""
    print("\n" + "=" * 60)
    print("ğŸ’¾ ë°ì´í„° ë³‘í•© ë° ì €ì¥")
    print("=" * 60)
    
    for week_id, week_data in weeks_data.items():
        week_info = week_data['info']
        new_posts = week_data['posts']
        
        # ê¸°ì¡´ ë°ì´í„° ë¡œë“œ
        existing_data = load_week_data(week_id)
        
        if existing_data:
            # ê¸°ì¡´ ê²Œì‹œê¸€ ID ì¶”ì¶œ
            existing_ids = {post['post_id'] for post in existing_data['posts']}
            
            # ìƒˆ ê²Œì‹œê¸€ë§Œ í•„í„°ë§
            unique_new_posts = [post for post in new_posts if post['post_id'] not in existing_ids]
            
            # ë³‘í•©
            all_posts = existing_data['posts'] + unique_new_posts
            
            print(f"ğŸ“¦ {week_id}: ê¸°ì¡´ {len(existing_data['posts'])}ê°œ + ì‹ ê·œ {len(unique_new_posts)}ê°œ = ì´ {len(all_posts)}ê°œ")
        else:
            # ìƒˆ íŒŒì¼ ìƒì„±
            all_posts = new_posts
            print(f"ğŸ†• {week_id}: ì‹ ê·œ íŒŒì¼ ìƒì„± (ì´ {len(all_posts)}ê°œ)")
        
        # ë‚ ì§œìˆœ ì •ë ¬ (ìµœì‹ ìˆœ)
        all_posts.sort(key=lambda x: x['datetime'], reverse=True)
        
        # ë°ì´í„° êµ¬ì¡° ìƒì„±
        week_structure = create_week_structure(week_info)
        week_structure['posts'] = all_posts
        week_structure['total_posts'] = len(all_posts)
        
        # ì €ì¥
        save_week_data(week_id, week_structure)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # í¬ë¡¤ë§
        posts = crawl_posts()
        
        if not posts:
            print("âš ï¸  ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ì£¼ì°¨ë³„ ë¶„ë¥˜
        weeks_data = organize_by_week(posts)
        
        # ë³‘í•© ë° ì €ì¥
        merge_and_save(weeks_data)
        
        print("\n" + "=" * 60)
        print("ğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        print("=" * 60)
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    main()
